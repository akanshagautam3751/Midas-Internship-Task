# Speech Emotion Problem

## Introduction
With the rapid growth of Artificial Intelligence (AI), multimodal emotion recognition has become a major research topic, primarily due to its potential applications in many challenging tasks, such as dialogue generation, user behavior understanding, multimodal interaction, and others. 
Predict the emotion of a human through his/her speech. It is a classical classification problem.

## Approach
### Data Visualisation
* Play a sound excerpt in the Jupyter Notebook using IPython.display
* To plot the wave diagram of the audio file, I've used librosa
* Displayed Spectogram
### Feature Extraction
We can use MFCC to be our input feature. Loading audio data and converting it to MFCCs format by the Python package librosa
### Training the model
The model has been trained on various Machine Learning Algorithms like Support Vector Machine, Decision Tree and Multi-layer Preceptron. 

# Refrences
* https://towardsdatascience.com/extract-features-of-music-75a3f9bc265d 
* https://data-flair.training/blogs/python-mini-project-speech-emotion-recognition/  

